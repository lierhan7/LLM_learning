# BERTå¾®è°ƒè®­ç»ƒé¡¹ç›®

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„BERTæ¨¡å‹å¾®è°ƒè®­ç»ƒé¡¹ç›®ï¼Œç”¨äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚ä»£ç ç»è¿‡ä¼˜åŒ–ï¼Œå…·æœ‰è‰¯å¥½çš„ç»“æ„åŒ–è®¾è®¡å’Œä¸°å¯Œçš„åŠŸèƒ½ã€‚

## é¡¹ç›®ç»“æ„

```
â”œâ”€â”€ trainer.py              # ä¸»è®­ç»ƒè„šæœ¬ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
â”œâ”€â”€ config.py               # é…ç½®ç®¡ç†
â”œâ”€â”€ utils.py                # å®ç”¨å·¥å…·å‡½æ•°
â”œâ”€â”€ run_training.py         # å‘½ä»¤è¡Œè¿è¡Œè„šæœ¬
â”œâ”€â”€ requirements.txt        # ä¾èµ–åŒ…åˆ—è¡¨
â””â”€â”€ README.md              # é¡¹ç›®è¯´æ˜
```

## ä¸»è¦ç‰¹æ€§

### ğŸš€ æ ¸å¿ƒåŠŸèƒ½
- **å®Œæ•´çš„è®­ç»ƒæµç¨‹**: æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°å’Œä¿å­˜
- **é…ç½®åŒ–ç®¡ç†**: çµæ´»çš„é…ç½®ç³»ç»Ÿï¼Œæ”¯æŒYAMLé…ç½®æ–‡ä»¶
- **è¯¦ç»†çš„æ—¥å¿—è®°å½•**: å®Œæ•´çš„è®­ç»ƒè¿‡ç¨‹è®°å½•
- **æ¨¡å‹è¯„ä¼°**: å¤šç§è¯„ä¼°æŒ‡æ ‡å’Œå¯è§†åŒ–

### ğŸ“Š é«˜çº§åŠŸèƒ½
- **æ—©åœæœºåˆ¶**: é˜²æ­¢è¿‡æ‹Ÿåˆ
- **å­¦ä¹ ç‡è°ƒåº¦**: çº¿æ€§å­¦ä¹ ç‡è¡°å‡
- **æ··åˆç²¾åº¦è®­ç»ƒ**: èŠ‚çœå†…å­˜ï¼ŒåŠ é€Ÿè®­ç»ƒ
- **æ¢¯åº¦ç´¯ç§¯**: æ”¯æŒå¤§æ‰¹æ¬¡è®­ç»ƒ
- **æŒ‡æ ‡è·Ÿè¸ª**: è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–

### ğŸ› ï¸ å®ç”¨å·¥å…·
- **GPUç›‘æ§**: å®æ—¶æ˜¾ç¤ºGPUä½¿ç”¨æƒ…å†µ
- **å‚æ•°ç»Ÿè®¡**: æ¨¡å‹å‚æ•°æ•°é‡ç»Ÿè®¡
- **å¯è§†åŒ–å·¥å…·**: è®­ç»ƒæ›²çº¿å’Œæ··æ·†çŸ©é˜µ
- **æ¨¡å‹ä¿å­˜**: è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. åŸºç¡€è®­ç»ƒ

```bash
# ä½¿ç”¨é»˜è®¤å‚æ•°è®­ç»ƒ
python run_training.py

# ä½¿ç”¨å¿«é€Ÿæµ‹è¯•æ¨¡æ¿
python run_training.py --template quick_test

# ä½¿ç”¨ç”Ÿäº§ç¯å¢ƒæ¨¡æ¿
python run_training.py --template production
```

### 3. è‡ªå®šä¹‰è®­ç»ƒ

```bash
# è‡ªå®šä¹‰å‚æ•°
python run_training.py --batch_size 16 --learning_rate 2e-5 --num_epochs 5

# ä½¿ç”¨é…ç½®æ–‡ä»¶
python run_training.py --config config_production.yaml
```

## é…ç½®ç³»ç»Ÿ

### ä½¿ç”¨é¢„å®šä¹‰æ¨¡æ¿

```python
from config import ConfigTemplates

# å¿«é€Ÿæµ‹è¯•é…ç½®
config = ConfigTemplates.quick_test()

# ç”Ÿäº§ç¯å¢ƒé…ç½®
config = ConfigTemplates.production()

# å¤§æ¨¡å‹é…ç½®
config = ConfigTemplates.large_model()
```

### åˆ›å»ºè‡ªå®šä¹‰é…ç½®

```python
from config import TrainingConfig

config = TrainingConfig(
    model_name="bert-base-uncased",
    batch_size=16,
    learning_rate=2e-5,
    num_epochs=5,
    output_dir="./my_results"
)

# ä¿å­˜é…ç½®
config.save_config("my_config.yaml")
```

### ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®

```python
config = TrainingConfig.from_yaml("my_config.yaml")
```

## é«˜çº§ä½¿ç”¨

### 1. ç›´æ¥ä½¿ç”¨è®­ç»ƒå™¨ç±»

```python
from trainer import TextClassificationTrainer
from config import TrainingConfig

config = TrainingConfig(
    model_name="bert-base-uncased",
    batch_size=8,
    num_epochs=3
)

trainer = TextClassificationTrainer(config)
metrics = trainer.train()
```

### 2. ä½¿ç”¨å®ç”¨å·¥å…·

```python
from utils import (
    set_seed, 
    count_parameters, 
    plot_training_history,
    EarlyStopping,
    MetricsTracker
)

# è®¾ç½®éšæœºç§å­
set_seed(42)

# ç»Ÿè®¡æ¨¡å‹å‚æ•°
params_info = count_parameters(model)
print(f"æ¨¡å‹å‚æ•°æ•°é‡: {params_info['total_params_M']:.1f}M")

# ä½¿ç”¨æŒ‡æ ‡è·Ÿè¸ªå™¨
tracker = MetricsTracker()
# ... è®­ç»ƒè¿‡ç¨‹ä¸­æ·»åŠ æŒ‡æ ‡ ...
tracker.plot_history("training_history.png")
```

### 3. æ—©åœæœºåˆ¶

```python
from utils import EarlyStopping

early_stopping = EarlyStopping(patience=3, min_delta=0.001)

for epoch in range(num_epochs):
    # ... è®­ç»ƒä»£ç  ...
    eval_score = evaluate_model()
    
    if early_stopping(eval_score, model):
        print("æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")
        break
```

## å‘½ä»¤è¡Œå‚æ•°

### åŸºæœ¬å‚æ•°
- `--model_name`: é¢„è®­ç»ƒæ¨¡å‹åç§° (é»˜è®¤: bert-base-uncased)
- `--batch_size`: æ‰¹æ¬¡å¤§å° (é»˜è®¤: 8)
- `--learning_rate`: å­¦ä¹ ç‡ (é»˜è®¤: 5e-5)
- `--num_epochs`: è®­ç»ƒè½®æ•° (é»˜è®¤: 3)

### æ•°æ®å‚æ•°
- `--dataset_name`: æ•°æ®é›†åç§° (é»˜è®¤: glue)
- `--dataset_config`: æ•°æ®é›†é…ç½® (é»˜è®¤: mrpc)
- `--max_length`: æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: 512)

### é«˜çº§å‚æ•°
- `--fp16`: å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
- `--warmup_steps`: é¢„çƒ­æ­¥æ•°
- `--eval_steps`: è¯„ä¼°é—´éš”æ­¥æ•°
- `--seed`: éšæœºç§å­

## è¾“å‡ºæ–‡ä»¶

è®­ç»ƒå®Œæˆåï¼Œä¼šåœ¨è¾“å‡ºç›®å½•ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š

```
results/
â”œâ”€â”€ best_model/              # æœ€ä½³æ¨¡å‹
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â””â”€â”€ tokenizer files...
â”œâ”€â”€ final_model/             # æœ€ç»ˆæ¨¡å‹
â”œâ”€â”€ training_config.yaml     # è®­ç»ƒé…ç½®
â”œâ”€â”€ training.log            # è®­ç»ƒæ—¥å¿—
â””â”€â”€ metrics.json            # è¯„ä¼°æŒ‡æ ‡
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### å†…å­˜ä¼˜åŒ–
- ä½¿ç”¨è¾ƒå°çš„batch_size
- å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (`--fp16`)
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯

### è®­ç»ƒåŠ é€Ÿ
- ä½¿ç”¨GPUè®­ç»ƒ
- å¢åŠ batch_sizeï¼ˆåœ¨å†…å­˜å…è®¸çš„æƒ…å†µä¸‹ï¼‰
- ä½¿ç”¨é¢„çƒ­å’Œå­¦ä¹ ç‡è°ƒåº¦

### æ¨¡å‹æ•ˆæœ
- å¢åŠ è®­ç»ƒè½®æ•°
- è°ƒæ•´å­¦ä¹ ç‡
- ä½¿ç”¨æ›´å¤§çš„é¢„è®­ç»ƒæ¨¡å‹

## å¸¸è§é—®é¢˜

### Q: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
A: 
1. å‡å°batch_size
2. å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (`--fp16`)
3. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
4. å‡å°max_length

### Q: è®­ç»ƒé€Ÿåº¦æ…¢æ€ä¹ˆåŠï¼Ÿ
A:
1. ç¡®ä¿ä½¿ç”¨GPU
2. å¢åŠ batch_size
3. å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
4. ä½¿ç”¨æ›´å¿«çš„æ•°æ®åŠ è½½

### Q: å¦‚ä½•é€‰æ‹©è¶…å‚æ•°ï¼Ÿ
A:
1. ä»é¢„å®šä¹‰æ¨¡æ¿å¼€å§‹
2. ä½¿ç”¨å¿«é€Ÿæµ‹è¯•æ¨¡æ¿è¿›è¡Œåˆæ­¥éªŒè¯
3. æ ¹æ®éªŒè¯é›†ç»“æœè°ƒæ•´å‚æ•°

## æ‰©å±•å»ºè®®

è¿™ä¸ªæ¡†æ¶å¯ä»¥å¾ˆå®¹æ˜“æ‰©å±•åˆ°å…¶ä»–ä»»åŠ¡ï¼š

1. **å…¶ä»–åˆ†ç±»ä»»åŠ¡**: ä¿®æ”¹æ•°æ®é›†å’Œæ ‡ç­¾æ•°é‡
2. **å¤šè¯­è¨€æ¨¡å‹**: ä½¿ç”¨å¤šè¯­è¨€é¢„è®­ç»ƒæ¨¡å‹
3. **å¤§å‹æ¨¡å‹**: ä½¿ç”¨BERT-largeæˆ–å…¶ä»–å¤§å‹æ¨¡å‹
4. **è‡ªå®šä¹‰æ•°æ®**: æ·»åŠ è‡ªå®šä¹‰æ•°æ®åŠ è½½å™¨

## è®¸å¯è¯

MIT License
